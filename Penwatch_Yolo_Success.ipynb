{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chugmn/NYP107Assign/blob/main/Penwatch_Yolo_Success.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9fM_zdIhpGt"
      },
      "source": [
        "# Object Detection using YOLO\n",
        "\n",
        " In this lab, we are going to learn how to train a pen and watch detector!\n",
        "\n",
        "At the end of this exercise, you will be able to:\n",
        "\n",
        "- create an object detection dataset in YOLO format\n",
        "- fine-tune a YOLOv8 pretrained model with the custom dataset\n",
        "- monitor the training progress and evaluation metrics\n",
        "- deploy the trained model for object detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWSUqz7gPOF4"
      },
      "source": [
        "## Create an object detection dataset\n",
        "\n",
        "We will use a sample pen and watch dataset to illustrate the process of annotation and packaging the dataset into different format for object detection (e.g. YOLO, Pascal VOC, COCO, etc).\n",
        "\n",
        "To annotate, there are many different tools available, such as the very basic [LabelImg](https://github.com/HumanSignal/labelImg) , or the more feature-packed tool such as [Label Studio](https://labelstud.io/), or online service such as [Roboflow](https://roboflow.com/).\n",
        "\n",
        "### Raw Image Dataset\n",
        "\n",
        "You can download the pen and watch images (without annotations) from this link:\n",
        "\n",
        "https://github.com/chugmn/NYP107Assign/raw/refs/heads/main/penorwatches.v2i.yolov8.zip\n",
        "\n",
        "Unzip the file to a local folder.\n",
        "\n",
        "There are total of 74 images. You should divide the images into both training and validation set (e.g. 80%-20%, i.e. 59 images for train, and 15 for test).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgMJJTcNQ7us"
      },
      "source": [
        "### Option 1: Label Studio\n",
        "\n",
        "You can follow the [steps](https://labelstud.io/guide/quick_start) here to setup Label Studio on your PC. It is recommended to setup a conda environment before you install the Label Studio.  \n",
        "\n",
        "Here are the steps that need to be done:\n",
        "1. Create a new Project\n",
        "2. Import the images into Label Studio\n",
        "3. Set up the Labelling UI tempalte (choose Object Detection with Bounding Box template)\n",
        "4. Export the dataset in YOLO format.\n",
        "\n",
        "The exported dataset will have the following folder structure:\n",
        "```\n",
        "<root folder>\n",
        "classes.txt    --> contains the labels, with each class label on a new line\n",
        "--images --> contains the images\n",
        "--labels --> contains the annotations (i.e. bbox coordinates)\n",
        "notes.json --> some info about this dataset (i.e. not used)\n",
        "```\n",
        "\n",
        "For training with YOLOv8 (from Ultralytics), you need to organize the files into `train` and `validate` (and optionally `test`) folders, and to create a `data.yaml` file to provide information about the folder location of test and validation set:\n",
        "\n",
        "```\n",
        "<root folder>\n",
        "--train\n",
        "----images\n",
        "----labels\n",
        "--valid\n",
        "----images\n",
        "----labels\n",
        "data.yaml\n",
        "```\n",
        "\n",
        "The data.yaml file should specify the following:\n",
        "```\n",
        "train: ../train/images\n",
        "val: ../valid/images\n",
        "test: ../test/images\n",
        "\n",
        "names:\n",
        "    0: pen and watch\n",
        "```\n",
        "\n",
        "If you have more than one class of object to detect, specify the rest of the names under the names field.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxOlLlBJb0kx"
      },
      "source": [
        "## Option 2: Roboflow\n",
        "\n",
        "Alternatively, you can use the online service Roboflow to do annotation. Roboflow integrates very well with Ultralytics and you can easily export the dataset in a format recognized by Ultralytics trainer (for YOLO model)\n",
        "\n",
        "You can create a new account with [Roboflow](https://roboflow.com/).\n",
        "\n",
        "Similarly, you can create a new project, upload all the raw images, annotate them and then export.\n",
        "\n",
        "You can choose the format to be YOLOv8 and choose local directory to download the dataset locally instead of pushing it to the Roboflow universal wish.\n",
        "\n",
        "Here is a [introductory blog](https://blog.roboflow.com/getting-started-with-roboflow/) on using the Roboflow to annotate.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPGO-z8rlqH8"
      },
      "source": [
        "## Auto Labelling using Grounding DINO\n",
        "\n",
        "Both Label Studio and Roboflow supports the use of Grounding DINO to auto label the dataset.\n",
        "\n",
        "[Grounding DINO](https://github.com/IDEA-Research/GroundingDINO) is open-set object detector, marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with human inputs (prompts) such as category names or referring expressions.\n",
        "\n",
        "### Using Grounding DINO with Label Studio\n",
        "\n",
        "You can follow the instruction [here](https://labelstud.io/blog/using-text-prompts-for-image-annotation-with-grounding-dino-and-label-studio/)  to setup the Grounding DINO ML backend to integrate with your label studio.\n",
        "\n",
        "### Using Grounding DINO with Roboflow\n",
        "\n",
        "Here is a [video tutorial](https://youtu.be/SDV6Gz0suAk) on using Grounding DINO with Roboflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_VTuEfec2YW"
      },
      "source": [
        "### Download Annotated Dataset\n",
        "\n",
        "\n",
        "We download and unzip to the directory called `datasets`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVh2OFXvtiM4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%%bash\n",
        "wget https://github.com/chugmn/NYP107Assign/raw/refs/heads/main/penorwatches.v2i.yolov8.zip\n",
        "mkdir -p datasets\n",
        "unzip penorwatches.v2i.yolov8.zip -d datasets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BTQ6QpZz3kP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ultralytics\n",
        "!pip install comet_ml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgyWGNT4-MLv"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "YOLOv8 comes with different sizes of pretrained models: yolov8n, yolov8s, .... They differs in terms of their sizes, inference speeds and mean average precision:\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/yolo-models.png?raw=true\" width=\"70%\"/>\n",
        "\n",
        "\n",
        "We will use the small pretrained model yolo8s and finetune it on our custom dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKnPWyDwUfvZ"
      },
      "source": [
        "### Setup the logging\n",
        "\n",
        "Ultralytics support logging to `wandb`, `comet.ml` and `tensorboard`, out of the box. Here we only enable wandb.\n",
        "\n",
        "You need to create an account at [`wandb`](https://wandb.ai) and get the API key from https://wandb.ai/authorize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbMwi27fRsyD",
        "outputId": "c1fd526c-9793-40dc-8d24-a30717e989e9"
      },
      "outputs": [],
      "source": [
        "from ultralytics import settings\n",
        "\n",
        "settings.update({\"wandb\": True,\n",
        "                 \"comet\": False,\n",
        "                 \"tensorboard\": False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq9gV4A1VHlq"
      },
      "source": [
        "### Training\n",
        "\n",
        "We specify the path to `data.yaml` file, and train with a batch size of 15, and we also save the checkpoint at each epoch (save_period=1). We assume here you are connected to a GPU, hence we can specify the device to use as `device=0` to select the first GPU.  We specify the project name as 'penandwatch', this will create a folder called `penandwatch` to store the weights and various training artifacts such as F1, PR curves, confusion matrics, training results (loss, mAP, etc).\n",
        "\n",
        "For a complete listing of train settings, you can see [here](https://docs.ultralytics.com/modes/train/#train-settings).\n",
        "\n",
        "You can also specify the type of data [augmentation](https://docs.ultralytics.com/modes/train/#augmentation-settings-and-hyperparameters)  you want as part of the train pipeline.\n",
        "\n",
        "You can monitor your training progress at wandb (the link is given in the train output below)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrnbwsmcpYE6",
        "outputId": "4a3de649-eb51-4727-a78c-ac304e994e19"
      },
      "outputs": [],
      "source": [
        "!ls -la datasets/valid/images | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NwjnKk02x-cF",
        "outputId": "ef8df8d7-5da0-4a88-8d43-5d5d64fdd883"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from ultralytics import settings\n",
        "\n",
        "model = YOLO(\"yolov8s.pt\")  # Load a pre-trained YOLO model\n",
        "result = model.train(data=\"datasets/data.yaml\",\n",
        "                     epochs=30,\n",
        "                     save_period=1,\n",
        "                     batch=16,\n",
        "                     device=\"cpu\",\n",
        "                     project='pensandwatches',\n",
        "                     plots=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59BONHriXz3H"
      },
      "source": [
        "You can see the various graphs in your wandb dashboard, for example:\n",
        "\n",
        "*metrics*\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/wandb-metrics.png?raw=true\"/>\n",
        "\n",
        "*Train and validation loss*\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti107-2024S2/blob/main/assets/wandb-loss.png?raw=true\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3QivGwNaY3l"
      },
      "source": [
        "You can go to the folder `penandwatch-->train-->weights` and you will files like epoch0.pt, epoch1.pt, .... and also best.pt.\n",
        "The epoch0.pt, epoch1.pt are the checkpoints that are saved every period (in our case, we specify period as 1 epoch).  The best.pt contains the best checkpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-PxfxzMbAOS"
      },
      "source": [
        "We can run the best model (using the best checkpoint) against the validation dataset to see the overall model performance on validation set.  \n",
        "\n",
        "You should see around `0.88` for `mAP50`, and `0.78` for `mAP50-95`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-n6GRS5f2f05",
        "outputId": "fda6846e-903e-45e7-e1dd-e032414bf83e"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"pensandwatches/train/weights/best.pt\")\n",
        "validation_results = model.val(data=\"datasets/data.yaml\", device=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bio2cKcnb-Z-"
      },
      "source": [
        "## Export and Deployment\n",
        "\n",
        "Your model is in pytorch format (.pt). You can export the model to various format, e.g. TorchScript, ONNX, OpenVINO, TensorRT, etc. depending on your use case, and deployment platform (e.g. CPU or GPU, etc)\n",
        "\n",
        "You can see the list of [supported formats](https://docs.ultralytics.com/modes/export/#export-formats)  and the option they support in terms of further optimization (such as imagesize, int8, half-precision, etc) in the ultralytics site.\n",
        "\n",
        "Ultralytics provide a utility function to benchmark your model using different supported formats automatically. You can uncomment the code in the following code cell to see the benchmark result. If you are benchmarking for CPU only, the change the `device=0` to `device='cpu'`.  \n",
        "\n",
        "**Beware: it will take quite a while to complete the benchmark**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usgkfg87bZBO"
      },
      "outputs": [],
      "source": [
        "# from ultralytics.utils.benchmarks import benchmark\n",
        "\n",
        "# # Benchmark on GPU (device=0 means the 1st GPU device)\n",
        "# benchmark(model=\"penandwatch/train/weights/best.pt\", data=\"datasets/data.yaml\", imgsz=640, half=False, device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD-vKnPYfBu3"
      },
      "source": [
        "\n",
        "In the following code, we export it as OpenVINO. OpenVINO is optimized for inference on Intel CPUs and since we will use the model later on to do inference on local Windows machine (which runs Intel chip), we will export it as OpenVINO format. We also specify using int8 quantization, which results in faster inference, at the cost of accuracy.\n",
        "\n",
        "For more information on OpenVINO, go to the [official documentation](https://docs.openvino.ai/2024/index.html).\n",
        "\n",
        "After export, you can find the openvino model in `penandwatch\\train\\weights\\best_openvino_model` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ccd1153680574edebad7ad8b77415800",
            "a6533ff3fd97479fb942ac01589c7b25",
            "4c72f06bfd904b44be0b46fe4c91c241",
            "59d17b94fcb14b04b5c220245afe0d13"
          ]
        },
        "id": "S6PDNf5NaPch",
        "outputId": "c55732ac-5bc1-4ef3-ab49-b9a4b33bdf07"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"pensandwatches/train/weights/best.pt\")\n",
        "exported_path = model.export(format=\"openvino\", int8=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r11lPCYrfMKN"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Let's test our model on some sample pictures. You can optionally specify the confidence threshold (e.g. `conf=0.5`), and the IoU (e.g. `iou=0.6`) for the NMS. The model will only output the bounding boxes of those detection that exceeds the confidence threshould and the IoU threshold.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ziKNFammhMxv",
        "outputId": "45e8c380-8f7d-44dd-b9bd-269d58ae163a"
      },
      "outputs": [],
      "source": [
        "#---\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "# Image URL\n",
        "url = 'https://raw.githubusercontent.com/chugmn/NYP107Assign/main/watchtest.jpeg'\n",
        "\n",
        "# Fetch the image\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    print(\"Image downloaded successfully!\")\n",
        "    # Open the image with PIL to check if it works\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    img.show()\n",
        "else:\n",
        "    print(\"Failed to download image. Status code:\", response.status_code)\n",
        "\n",
        "\n",
        "#---\n",
        "import cv2\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "# Path where the image is saved\n",
        "image_path = '/content/watchtest.jpeg'\n",
        "\n",
        "# Download the image\n",
        "url = 'https://raw.githubusercontent.com/chugmn/NYP107Assign/main/watchtest.jpeg'\n",
        "img_data = requests.get(url).content\n",
        "with open(image_path, 'wb') as f:\n",
        "    f.write(img_data)\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "\n",
        "if img is None:\n",
        "    print(\"Image not read correctly!\")\n",
        "else:\n",
        "    print(\"Image loaded successfully!\")\n",
        "\n",
        "# Show the image (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "#\n",
        "\n",
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "\n",
        "#source = 'https://raw.githubusercontent.com/nyp-sit/iti107-2024S2/refs/heads/main/session-3/samples/sample_penandwatch.jpeg'\n",
        "#source = './samples/sample_penandwatch.jpeg'\n",
        "source = 'https://raw.githubusercontent.com/chugmn/NYP107Assign/main/watchtest.jpeg'\n",
        "model = YOLO(\"pensandwatches/train/weights/best_int8_openvino_model\", task='detect')\n",
        "result = model(source, conf=0.5, iou=0.6)\n",
        "model = YOLO(\"pensandwatches/train/weights/best_int8_openvino_model\", task='detect')\n",
        "result = model(source, conf=0.5, iou=0.6)\n",
        "# Visualize the results\n",
        "for i, r in enumerate(result):\n",
        "    print(r)\n",
        "    # Plot results image\n",
        "    im_bgr = r.plot()  # BGR-order numpy array\n",
        "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
        "\n",
        "    # Show results to screen (in supported environments)\n",
        "    r.show()\n",
        "\n",
        "    # Save results to disk\n",
        "    r.save(filename=f\"results{i}.jpg\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Pen detection\n",
        "\n",
        "\n",
        "import cv2\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "# Path where the image is saved\n",
        "image_path = '/content/pentest.jpeg'\n",
        "\n",
        "# Download the image\n",
        "url = 'https://raw.githubusercontent.com/chugmn/NYP107Assign/main/pentest.jpeg'\n",
        "img_data = requests.get(url).content\n",
        "with open(image_path, 'wb') as f:\n",
        "    f.write(img_data)\n",
        "\n",
        "# Read the image using OpenCV\n",
        "img = cv2.imread(image_path)\n",
        "\n",
        "if img is None:\n",
        "    print(\"Image not read correctly!\")\n",
        "else:\n",
        "    print(\"Image loaded successfully!\")\n",
        "\n",
        "# Show the image (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "#\n",
        "\n",
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "\n",
        "#source = 'https://raw.githubusercontent.com/nyp-sit/iti107-2024S2/refs/heads/main/session-3/samples/sample_balloon.jpeg'\n",
        "#source = './samples/sample_penanwatch.jpeg'\n",
        "source = 'https://raw.githubusercontent.com/chugmn/NYP107Assign/main/pentest.jpeg'\n",
        "model = YOLO(\"pensandwatches/train/weights/best_int8_openvino_model\", task='detect')\n",
        "result = model(source, conf=0.5, iou=0.6)\n",
        "\n",
        "# Visualize the results\n",
        "for i, r in enumerate(result):\n",
        "    print(r)\n",
        "    # Plot results image\n",
        "    im_bgr = r.plot()  # BGR-order numpy array\n",
        "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
        "\n",
        "    # Show results to screen (in supported environments)\n",
        "    r.show()\n",
        "\n",
        "    # Save results to disk\n",
        "    r.save(filename=f\"results{i}.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygXaw7lxu8HT"
      },
      "source": [
        "## Download the Model\n",
        "\n",
        "If you are training your model on Google Colab, you will download the exported OpenVINO model to a local PC. If you are training your model locally, then the exported model should already be on your local PC.\n",
        "\n",
        "Run the following code to zip up the OpenVINO folder and download to local PC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTTV-ndvuOx7"
      },
      "source": [
        "*Note: If you encountered error message \"NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\", uncomment the following cell and run it.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O-nmcYRuMIg"
      },
      "outputs": [],
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_CI0ZBTteA2",
        "outputId": "1e2d2e5d-cf0e-4aee-ff3a-13e8e9e3adea"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "mv ./pensandwatches/train/weights/best_int8_openvino_model/ .\n",
        "zip -r best_int8_openvino_model.zip best_int8_openvino_model\n",
        "\n",
        "# Now go to best_openvino_model to download the best_openvino_model.zip file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui8i_pB5sItu"
      },
      "source": [
        "## Streaming\n",
        "\n",
        "We can also do real-time detection on a video or camera steram.\n",
        "\n",
        "The code below uses openCV library to display video in a window, and can only be run locally on a local laptop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmzL3StT5_CI"
      },
      "source": [
        "### Video File\n",
        "\n",
        "You need `OpenCV` to run the following code.  In your conda environment, install `opencv` for python using the following command:\n",
        "\n",
        "```\n",
        "pip3 install opencv-python\n",
        "```\n",
        "or\n",
        "```\n",
        "conda install opencv\n",
        "```\n",
        "\n",
        "Let's donwload the sample video file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNK13LNd87G4",
        "outputId": "f4c12c98-ef7f-413a-ef47-1b2afc54a7f3"
      },
      "outputs": [],
      "source": [
        "!wget https://githubusercontent.com/chugmn/NYP107Assign/raw/refs/heads/main/penwatch.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nvf5BvM9_q2"
      },
      "source": [
        "### Streaming and display video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGWz9dPSMHzl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZoRxssK98O_"
      },
      "source": [
        "### Detect and write to a video file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c31f1819def147cabff3297cc988196c",
            "8e223f77d5b64768989334ae66c9676b",
            "2e4b24cb3ef444dd8db4f1d11c4f7dc4",
            "2f85fb4da7a34bea99d0fc63066d89af",
            "605e69c83d9f4e0ca208f6b46889aecb",
            "0bf494af84b14217b48df808b6c6668e",
            "3e6e5fe8b7994408ad964fcd78b7ad1f",
            "88b26fa997ef4fcb87bc41b48758b363",
            "f01cf0fd031a4f019ff0c5e91d7e9301",
            "21277fe0f2eb4325aee6a938ce171a42",
            "4257959e095c47a989db7eecbfe525bf"
          ]
        },
        "id": "4Sr-_DGxyRT7",
        "outputId": "3649b3aa-9546-47aa-e4d8-6504be9c763a"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "\n",
        "def write_video(video_in_filepath, video_out_filepath, model, conf=0.5, device=\"cpu\"):\n",
        "    # Open the video file\n",
        "    video_reader = cv2.VideoCapture(video_in_filepath)\n",
        "\n",
        "    if not video_reader.isOpened():\n",
        "        print(f\"Error: Unable to open input video file {video_in_filepath}\")\n",
        "        return\n",
        "\n",
        "    nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    fps = video_reader.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    # Initialize video writer\n",
        "    video_writer = cv2.VideoWriter(\n",
        "        video_out_filepath,\n",
        "        cv2.VideoWriter_fourcc(*'mp4v'),  # Codec for MP4\n",
        "        fps,\n",
        "        (frame_w, frame_h)\n",
        "    )\n",
        "\n",
        "    # Loop through the video frames\n",
        "    for _ in tqdm(range(nb_frames), desc=\"Processing video frames\"):\n",
        "        # Read a frame from the video\n",
        "        success, frame = video_reader.read()\n",
        "\n",
        "        if not success:\n",
        "            break  # Break loop if end of video is reached\n",
        "\n",
        "        # Run YOLO inference on the frame\n",
        "        results = model(frame, conf=conf, device=device)\n",
        "\n",
        "        # Visualize the results on the frame\n",
        "        annotated_frame = results[0].plot()\n",
        "\n",
        "        # Write the annotated frame to the output video\n",
        "        video_writer.write(annotated_frame)\n",
        "\n",
        "    # Release resources\n",
        "    video_reader.release()\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    print(f\"Processed video saved to: {video_out_filepath}\")\n",
        "\n",
        "def display_video(video_filepath):\n",
        "    \"\"\"Display the processed video in Colab.\"\"\"\n",
        "    cap = cv2.VideoCapture(video_filepath)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Unable to open video file {video_filepath}\")\n",
        "        return\n",
        "\n",
        "    while cap.isOpened():\n",
        "        success, frame = cap.read()\n",
        "        if not success:\n",
        "            break\n",
        "        cv2_imshow(frame)  # Use cv2_imshow to display frames in Colab\n",
        "        # Add a break condition to avoid flooding Colab with too many frames\n",
        "        key = cv2.waitKey(1)\n",
        "        if key == ord(\"q\"):  # Simulate pressing 'q' to quit\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    print(\"Video display completed.\")\n",
        "\n",
        "# Input and output video paths\n",
        "video_in_file = \"penwatch.mp4\"\n",
        "basename = Path(video_in_file).stem\n",
        "video_out_file = os.path.join(basename + '_detected.mp4')\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO(\"best_int8_openvino_model\", task=\"detect\")\n",
        "\n",
        "# Process the video\n",
        "write_video(video_in_file, video_out_file, model, conf=0.5, device=\"cpu\")\n",
        "\n",
        "# Display the processed video\n",
        "display_video(video_out_file)\n",
        "\n",
        "# Optionally, download the processed video\n",
        "files.download(video_out_file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bf494af84b14217b48df808b6c6668e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21277fe0f2eb4325aee6a938ce171a42": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e4b24cb3ef444dd8db4f1d11c4f7dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88b26fa997ef4fcb87bc41b48758b363",
            "max": 1906,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f01cf0fd031a4f019ff0c5e91d7e9301",
            "value": 1906
          }
        },
        "2f85fb4da7a34bea99d0fc63066d89af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21277fe0f2eb4325aee6a938ce171a42",
            "placeholder": "​",
            "style": "IPY_MODEL_4257959e095c47a989db7eecbfe525bf",
            "value": " 1906/1906 [13:46&lt;00:00,  2.64it/s]"
          }
        },
        "3e6e5fe8b7994408ad964fcd78b7ad1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4257959e095c47a989db7eecbfe525bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c72f06bfd904b44be0b46fe4c91c241": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_59d17b94fcb14b04b5c220245afe0d13",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Applying Fast Bias correction <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #0068b5; text-decoration-color: #0068b5\">63/63</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:05</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:00</span>\n</pre>\n",
                  "text/plain": "Applying Fast Bias correction \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m63/63\u001b[0m • \u001b[38;2;0;104;181m0:00:05\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "59d17b94fcb14b04b5c220245afe0d13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "605e69c83d9f4e0ca208f6b46889aecb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88b26fa997ef4fcb87bc41b48758b363": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e223f77d5b64768989334ae66c9676b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bf494af84b14217b48df808b6c6668e",
            "placeholder": "​",
            "style": "IPY_MODEL_3e6e5fe8b7994408ad964fcd78b7ad1f",
            "value": "Processing video frames: 100%"
          }
        },
        "a6533ff3fd97479fb942ac01589c7b25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c31f1819def147cabff3297cc988196c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e223f77d5b64768989334ae66c9676b",
              "IPY_MODEL_2e4b24cb3ef444dd8db4f1d11c4f7dc4",
              "IPY_MODEL_2f85fb4da7a34bea99d0fc63066d89af"
            ],
            "layout": "IPY_MODEL_605e69c83d9f4e0ca208f6b46889aecb"
          }
        },
        "ccd1153680574edebad7ad8b77415800": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a6533ff3fd97479fb942ac01589c7b25",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Statistics collection <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #0068b5; text-decoration-color: #0068b5\">4/4</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:05</span> • <span style=\"color: #0068b5; text-decoration-color: #0068b5\">0:00:00</span>\n</pre>\n",
                  "text/plain": "Statistics collection \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m4/4\u001b[0m • \u001b[38;2;0;104;181m0:00:05\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m\n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "f01cf0fd031a4f019ff0c5e91d7e9301": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
